{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Métricas\n",
    "\n",
    "El accuracy (vamos a traducirlo como exactitud) no siempre es una buena métrica para evaluar el rendimiento de un modelo de clasificación, especialmente en situaciones en las que los datos están desequilibrados o cuando los errores de diferentes clases tienen un impacto desigual.\n",
    "\n",
    "Veamos estos casos más a detalle\n",
    "\n",
    "1.Datos desequilibrados: Cuando las clases en el conjunto de datos están desequilibradas, es decir, una o varias clases tienen muchos más ejemplos que otras, el accuracy puede ser alto incluso si el modelo es deficiente en la predicción de la clase minoritaria. Esto se debe a que el modelo tiende a inclinarse hacia la clase mayoritaria y puede pasar por alto la clase minoritaria.\n",
    "\n",
    "2.Costos desiguales de errores: En algunas aplicaciones, los errores de predicción en ciertas clases pueden ser más costosos que en otras. Por ejemplo, en un sistema de detección de fraudes, es más grave clasificar un caso de fraude como legítimo (falso negativo) que clasificar un caso legítimo como fraude (falso positivo).\n",
    "\n",
    "3.Sesgo en las predicciones: En algunos casos, el modelo puede tener un sesgo sistemático hacia ciertas clases y realizar predicciones incorrectas con mayor frecuencia para esas clases. El accuracy no capturará este sesgo y puede dar una impresión falsa de un buen rendimiento general.\n",
    "\n",
    "Para realizar un análisis de resultados más detallado se pueden utilizar varias otras métricas, en este caso vamos a estudiar la matriz de confusión y el F1 score.\n",
    "\n",
    "\n",
    "##### Matriz de confusión\n",
    "\n",
    "La matriz de confusión es una herramienta útil para evaluar el rendimiento de un modelo de clasificación al mostrar la distribución de las predicciones en relación con las clases reales.\n",
    "\n",
    "Tiene una estructura cuadrada donde las filas representan las clases reales y las columnas representan las clases predichas.\n",
    "\n",
    "Para el caso específico de un problema de clasificación binaria tiene cuatro elementos:\n",
    "\n",
    "Verdaderos positivos (True Positives, TP): el número de muestras que se clasificaron correctamente como positivas.\n",
    "\n",
    "Falsos positivos (False Positives, FP): el número de muestras que se clasificaron incorrectamente como positivas.\n",
    "\n",
    "Falsos negativos (False Negatives, FN): el número de muestras que se clasificaron incorrectamente como negativas.\n",
    "\n",
    "Verdaderos negativos (True Negatives, TN): el número de muestras que se clasificaron correctamente como negativas.\n",
    "\n",
    "A partir de estos elementos, se pueden calcular varias métricas adicionales, como precisión, exhaustividad y F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Entrenando el Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriz de confusión:\n",
    "[[ 8  0  0]\n",
    " [ 0 11  0]\n",
    " [ 0  2  9]]\n",
    "\n",
    "\n",
    "Recordando el mapa de índices numéricos a categorías\n",
    "\n",
    "setosa: 0\n",
    "versicolon: 1\n",
    "virginica: 2\n",
    "\n",
    "En este ejemplo, el Random Forest realizó 8 predicciones correctas para la clase 0, 11 predicciones correctas para la clase 1 y 9 predicciones correctas para la clase 2, sin embargo clasificó erroneamente 2 ejemplos de la clase 2 como la clase 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 Score\n",
    "\n",
    "Para comprender el F1 score, primero debemos ver las métricas Precision y Recall.\n",
    "\n",
    "Sea una matriz de confusión binaria:\n",
    "\n",
    "Clase positiva\tClase negativa\n",
    "Predicho\tTP\tFP\n",
    "No predicho\tFN\tTN\n",
    "Dónde:\n",
    "\n",
    "�\n",
    "�\n",
    "TP: True positives (Verdaderos positivos)\n",
    "�\n",
    "�\n",
    "FP: False positives (Falsos positivos)\n",
    "�\n",
    "�\n",
    "FN: False negatives (Falsos negativos)\n",
    "�\n",
    "�\n",
    "TN: True negatives (Verdaderos negativos)\n",
    "Precision (Precisión): La precisión es una métrica que mide la proporción de muestras clasificadas correctamente como positivas (verdaderos positivos) con respecto a todas las muestras clasificadas como positivas (tanto verdaderos positivos como falsos positivos). En otras palabras, la precisión es una medida de la exactitud de las predicciones positivas.\n",
    "\n",
    "La fórmula para calcular la precisión es:\n",
    "\n",
    "Precision\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "Precision= \n",
    "TP+FP\n",
    "TP\n",
    "​\n",
    " \n",
    "Recall (Exhaustividad): El recall, también conocido como exhaustividad o sensibilidad, es una métrica que mide la proporción de muestras clasificadas correctamente como positivas (verdaderos positivos) con respecto a todas las muestras reales que son positivas (tanto verdaderos positivos como falsos negativos). El recall es una medida de la capacidad del modelo para identificar correctamente todas las muestras positivas.\n",
    "\n",
    "La fórmula para calcular el recall es:\n",
    "\n",
    "Recall\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "Recall= \n",
    "TP+FN\n",
    "TP\n",
    "​\n",
    " \n",
    "F1-score: El F1-score es una métrica que combina la precisión y el recall en un solo valor. Es la media armónica de la precisión y el recall, y proporciona un equilibrio entre ambas métricas. El F1-score es útil cuando hay un desequilibrio entre las clases o cuando se desea tener en cuenta tanto la precisión como el recall.\n",
    "\n",
    "La fórmula para calcular el F1-score es:\n",
    "\n",
    "�\n",
    "1\n",
    "score\n",
    "=\n",
    "2\n",
    "⋅\n",
    "Precision\n",
    "⋅\n",
    "Recall\n",
    "Precision\n",
    "+\n",
    "Recall\n",
    "F1 \n",
    "score\n",
    "​\n",
    " = \n",
    "Precision+Recall\n",
    "2⋅Precision⋅Recall\n",
    "​\n",
    " \n",
    "Veamos un ejemplo\n",
    "\n",
    "Clase positiva\tClase negativa\n",
    "Predicho\t90\t10\n",
    "No predicho\t20\t80\n",
    "Podemos calcular las métricas de la siguiente manera:\n",
    "\n",
    "Precision\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "=\n",
    "90\n",
    "90\n",
    "+\n",
    "10\n",
    "=\n",
    "0.9\n",
    "Recall\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "=\n",
    "90\n",
    "90\n",
    "+\n",
    "20\n",
    "=\n",
    "0.818\n",
    "�\n",
    "1\n",
    "score\n",
    "=\n",
    "2\n",
    "⋅\n",
    "Precision\n",
    "⋅\n",
    "Recall\n",
    "Precision\n",
    "+\n",
    "Recall\n",
    "=\n",
    "2\n",
    "⋅\n",
    "0.9\n",
    "⋅\n",
    "0.818\n",
    "0.9\n",
    "+\n",
    "0.818\n",
    "=\n",
    "0.857\n",
    "Precision\n",
    "Recall\n",
    "F1 \n",
    "score\n",
    "​\n",
    " \n",
    "​\n",
    "  \n",
    "= \n",
    "TP+FP\n",
    "TP\n",
    "​\n",
    " = \n",
    "90+10\n",
    "90\n",
    "​\n",
    " =0.9\n",
    "= \n",
    "TP+FN\n",
    "TP\n",
    "​\n",
    " = \n",
    "90+20\n",
    "90\n",
    "​\n",
    " =0.818\n",
    "= \n",
    "Precision+Recall\n",
    "2⋅Precision⋅Recall\n",
    "​\n",
    " = \n",
    "0.9+0.818\n",
    "2⋅0.9⋅0.818\n",
    "​\n",
    " =0.857\n",
    "​\n",
    " \n",
    "En este ejemplo, la precisión es 0.9, lo que significa que el 90% de las predicciones positivas son correctas. El recall es 0.818, lo que indica que el modelo identifica correctamente el 81.8% de todas las muestras positivas. El F1-score es 0.857\n",
    "\n",
    "Estas métricas son útiles para evaluar el rendimiento de un modelo de clasificación binaria y proporcionan información sobre la calidad de las predicciones positivas y la capacidad del modelo para identificar correctamente todas las muestras positivas.\n",
    "\n",
    "Sklearn permite extender esta métrica de diferentes formas usando el parámetro average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculando f1_score para cada clase\n",
    "scores = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "print(\"F1 scores:\", scores)\n",
    "\n",
    "\"\"\" F1 scores: [1.         0.91666667 0.9       ]\n",
    "El vector scores contiene los f1-scores para cada clase. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
