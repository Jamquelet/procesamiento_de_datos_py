{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging\n",
    "\n",
    "El bagging, también conocido como ensamble por votación con reemplazo bootstrap, es una técnica utilizada para mejorar la precisión y la estabilidad de los modelos predictivos. El término \"bagging\" es una abreviatura de \"bootstrap aggregating\".\n",
    "\n",
    "El bagging se basa en la idea de generar múltiples muestras de entrenamiento mediante el muestreo aleatorio con reemplazo de los datos originales. Cada muestra se utiliza para entrenar un modelo de aprendizaje en paralelo e independientemente. Luego, los modelos individuales se combinan para obtener una predicción final mediante un proceso de votación o promedio.\n",
    "\n",
    "El proceso de bagging se puede resumir en los siguientes pasos:\n",
    "\n",
    "1.Muestreo bootstrap: Se generan múltiples conjuntos de entrenamiento mediante muestreo aleatorio con reemplazo de los datos originales. Esto implica seleccionar aleatoriamente ejemplos del conjunto de datos de entrenamiento y permitir que un ejemplo se seleccione varias veces o que no se seleccione en absoluto.\n",
    "\n",
    "2.Entrenamiento de modelos individuales: Para cada conjunto de entrenamiento generado, se entrena un modelo de aprendizaje automático utilizando un algoritmo específico, como árboles de decisión, SVM (máquinas de vectores de soporte) o redes neuronales. Cada modelo se entrena de manera independiente y no se comparte información entre ellos.\n",
    "\n",
    "3.Predicción por votación o promediado: Una vez que todos los modelos se han entrenado, se realiza una predicción en conjunto para un nuevo ejemplo de prueba. En el caso de la clasificación, se puede utilizar una votación mayoritaria, donde cada modelo emite su propia predicción y la clase más votada se selecciona como predicción final. En la regresión, se puede realizar un promedio de las predicciones de los modelos para obtener una estimación final.\n",
    "\n",
    "La idea clave detrás del bagging es que al combinar múltiples modelos independientes, se reducen los errores individuales y se mejora la precisión general. El bagging ayuda a reducir el sobreajuste y la varianza al proporcionar una promedio o consenso de las predicciones de varios modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementación\n",
    "\n",
    "#Asumiendo que ya cargamos el dataset de iris\n",
    "\n",
    "#1.Importar las librerías necesarias:\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#2.Crear una instancia del árbol de decisión:\n",
    "base_tree = DecisionTreeClassifier()\n",
    "\n",
    "#3.Crear una instancia del clasificador Bagging y configurarlo para usar árboles de decisión como modelos base:\n",
    "bagging = BaggingClassifier(base_estimator=base_tree, n_estimators=10, random_state=42)\n",
    "#En este ejemplo, se utilizan 10 árboles de decisión como modelos base. Puedes ajustar el número de estimadores según sea necesario.\n",
    "\n",
    "#4.Ajustar el clasificador Bagging a los datos de entrenamiento:\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "#5.Realizar predicciones en el conjunto de prueba:\n",
    "y_pred = bagging.predict(X_test)\n",
    "\n",
    "#Evaluar la precisión del modelo:\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisión del modelo:\", accuracy)\n",
    "\n",
    "#Precisión del modelo: 0.9333333333333333\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
