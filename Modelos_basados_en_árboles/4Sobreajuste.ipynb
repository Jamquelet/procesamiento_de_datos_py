{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El sobreajuste, también conocido como sobreentrenamiento, es un fenómeno común en el aprendizaje automático en el cual un modelo se ajusta demasiado a los datos de entrenamiento y no generaliza bien a nuevos datos. En otras palabras, el modelo se \"memoriza\" los ejemplos de entrenamiento en lugar de capturar las características subyacentes que se aplican a datos no vistos.\n",
    "\n",
    "Los árboles de decisión pueden ser propensos al sobreajuste debido a su capacidad intrínseca para construir estructuras complejas y profundas. A medida que un árbol de decisión crece, puede adaptarse excesivamente a las peculiaridades y el ruido presentes en los datos de entrenamiento, lo que resulta en un rendimiento deficiente en la clasificación de nuevos ejemplos.\n",
    "\n",
    "Para abordar el problema del sobreajuste en los árboles de decisión, se pueden utilizar diversas estrategias:\n",
    "\n",
    "1.Poda (pruning): Consiste en recortar el árbol después de su construcción, eliminando nodos o ramas que no aportan beneficios significativos en términos de mejora de la precisión. Esto ayuda a simplificar la estructura del árbol y evita que se adapte excesivamente a los datos de entrenamiento.\n",
    "\n",
    "2.Limitar la profundidad máxima: Se puede establecer una restricción en la profundidad máxima del árbol, evitando que se vuelva demasiado complejo y específico para los datos de entrenamiento.\n",
    "\n",
    "3.Establecer el número mínimo de ejemplos en los nodos: Se puede definir un umbral mínimo para el número de ejemplos requeridos en cada nodo para permitir una partición. Esto evita divisiones innecesarias en casos donde el número de ejemplos es demasiado bajo para ser representativo.\n",
    "\n",
    "4.Uso de validación cruzada: Al dividir los datos de entrenamiento en conjuntos de entrenamiento y validación, se puede evaluar el rendimiento del árbol de decisión en datos no vistos durante la construcción. Esto ayuda a seleccionar el mejor modelo que generalice bien a nuevos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenando un Árbol de decisión\n",
    "\n",
    "#1.Importar las bibliotecas necesarias:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.Cargar el dataset de iris:\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "#3.Dividir el conjunto de datos en conjuntos de entrenamiento y prueba:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)\n",
    "#Esto divide los datos en un 80% para entrenamiento y un 20% para prueba.\n",
    "\n",
    "#4.Crear una instancia del árbol de decisión:\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "#5.Ajustar el árbol de decisión a los datos de entrenamiento:\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "#6.Realizar predicciones en el conjunto de prueba:\n",
    "y_pred_train = tree.predict(X_train)\n",
    "y_pred_test = tree.predict(X_test)\n",
    "\n",
    "#7.Evaluar la precisión del modelo:\n",
    "print(\"Precisión del modelo en train:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Precisión del modelo en test:\", accuracy_score(y_test, y_pred_test))\n",
    "\n",
    "print(\"Profundidad:\", tree.get_depth())\n",
    "print(\"Hojas:\", tree.get_n_leaves())\n",
    "\n",
    "\"\"\" Precisión del modelo en train: 1.0\n",
    "Precisión del modelo en test: 0.9333333333333333\n",
    "Profundidad: 5\n",
    "Hojas: 8 \"\"\"\n",
    "\n",
    "#En este caso no tenemos un claro sobreajuste, sin embargo podemos jugar con los parámetros para obtener un árbol más pequeño con un rendimiento similar:\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3, min_samples_leaf=1)\n",
    "\"\"\" Esto limita la profundidad máxima del árbol a 3 y requiere al menos 1 ejemplos en cada nodo hoja.\n",
    "\n",
    "Al ajustar estos parámetros, se puede regular la complejidad del árbol y reducir la tendencia al sobreajuste. Es importante encontrar un equilibrio adecuado, ya que una restricción excesiva puede llevar a un sesgo alto y un rendimiento deficiente en la predicción. Por lo tanto, es recomendable experimentar con diferentes valores de estos parámetros y utilizar técnicas de validación cruzada para evaluar el rendimiento general del modelo. \"\"\"\n",
    "\n",
    "\"\"\" Los resultados obtenidos con esta nueva configuración son:\n",
    "Precisión del modelo en train: 0.9833333333333333\n",
    "Precisión del modelo en test: 0.9333333333333333\n",
    "Profundidad: 3\n",
    "Hojas: 5\n",
    " \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
